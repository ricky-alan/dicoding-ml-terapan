# -*- coding: utf-8 -*-
"""movie-recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SB6I7gBWeFAqe7nE0dV4hTtiFlCttxxI

### Data Understanding
"""

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""Dataset ini terdiri dari 6 file terpisah, tapi yang saya gunakan pada proyek ini hanya 2 file saja, yaitu movie.csv dan rating.csv.

### Loading Data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model

movies = pd.read_csv('/kaggle/input/movielens-20m-dataset/movie.csv')
ratings = pd.read_csv('/kaggle/input/movielens-20m-dataset/rating.csv')

"""### Univariate Exploratory Data Analysis

movies: merupakan informasi mengenai film.  
ratings: merupakan rating film yang diberikan oleh user.

Movies
"""

movies.head()

print('Jumlah data film: ', len(movies['movieId'].unique()))

genres = movies['genres'].str.split('|').explode()

print('Jumlah genre film: ', len(genres.unique()))
genres.value_counts(ascending=True).plot(kind='barh')

"""Drama adalah genre film terbanyak, disusul oleh Comedy, Thriller, Romance, Action dan seterusnya.

Ratings
"""

ratings.head()

print('Jumlah user yang memberikan penilaian: ', len(ratings['userId'].unique()))
print('Jumlah film yang pernah dinilai user: ', len(ratings['movieId'].unique()))
print('Jumlah data penilaian film: ', len(ratings))

ratings.describe().T

"""Nilai rating terendah adalah 0.5, sedangkan nilai rating tertinggi adalah 5.0.

### Data Preprocessing

Menggabungkan dataframe movies dan ratings
"""

df_fix = pd.merge(movies, ratings, on='movieId', how='left')
df_fix

"""### Data Preparation

Drop kolom timestamp karena tidak diperlukan
"""

df_fix.drop('timestamp', axis=1, inplace=True)

"""Cek Missing Value"""

df_fix.isnull().sum()

"""Terdapat 534 baris missing value pada kolom userId dan rating. Karena jumlah ini tidak banyak dibandingkan dengan total data yang ada, data ini akan di-drop."""

df_fix.dropna(inplace=True)

"""### Model Content Based Filtering

Membuat dataframe baru untuk data film yang terdiri dari id, title, dan genres.
"""

movie_df = pd.DataFrame({
    'id': df_fix['movieId'].tolist(),
    'title': df_fix['title'].tolist(),
    'genres': df_fix['genres'].tolist()
})

"""Drop data film yang duplikat"""

movie_df = movie_df.drop_duplicates('id')
movie_df.head()

"""Membuat matriks genre untuk setiap film"""

movie_df['genres'] = movie_df['genres'].str.split('|')

for index, data in movie_df.iterrows():
  for genre in data['genres']:
    movie_df.at[index, genre] = 1

movie_df = movie_df.fillna(0)
genres_matrix = movie_df.loc[:, 'Adventure':]
genres_matrix.head()

"""Menghitung cosine similarity antar film"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(genres_matrix)
cosine_sim_df = pd.DataFrame(cosine_sim, index=movie_df['title'], columns=movie_df['title'])
cosine_sim_df.sample(5, axis=1).sample(5, axis=0)

"""Mendapatkan rekomendasi film"""

def movie_recommender(movie_name, similarity_data=cosine_sim_df, items=movie_df[['title', 'genres']], k=20):
  index = similarity_data.loc[:, movie_name].to_numpy().argpartition(range(-1, -k, -1))
  closest = similarity_data.iloc[index[-1:-(k+2):-1]][movie_name]
  closest = closest.drop(movie_name, errors='ignore')
  return pd.DataFrame(closest).merge(items, on='title').head(k).rename({movie_name: 'cosine_sim'}, axis=1)

"""Film pertama"""

movie = 'Toy Story (1995)'
movie_df[movie_df['title'].eq(movie)].loc[:, :'genres']

movie_recommender(movie)

"""Model dapat memberikan rekomendasi yang sangat baik dengan nilai similarity diatas 0.9.

Film kedua
"""

movie = '13Hrs (2010)'
movie_df[movie_df['title'].eq(movie)].loc[:, :'genres']

movie_recommender(movie)

"""Model dapat memberikan rekomendasi yang cukup baik dengan nilai silimarity diatas 0.7. Nilai ini lebih rendah dari nilai rekomendasi untuk film pertama. Hal ini dapat disebabkan karena data yang tidak seimbang.

### Model Collaborative Filtering

Shuffling data
"""

df_fix = df_fix.sample(frac=1, random_state=42)
df_fix

"""Encoding data user"""

user_ids = df_fix['userId'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}
df_fix['user'] = df_fix['userId'].map(user_to_user_encoded)
num_users = len(user_to_user_encoded)

"""Encoding data film"""

movie_ids = df_fix['userId'].unique().tolist()
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}
df_fix['movie'] = df_fix['movieId'].map(movie_to_movie_encoded)
num_movies = len(movie_to_movie_encoded)

"""Split data, 90% untuk training dan 10% untuk validation."""

min_rating = min(df_fix['rating'])
max_rating = max(df_fix['rating'])

x = df_fix[['user', 'movie']].values
y = df_fix['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_size = int(0.9 * df_fix.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_size],
    x[train_size:],
    y[:train_size],
    y[train_size:]
)

train_inputs = [x_train[:, 0], x_train[:, 1]]
val_inputs = [x_val[:, 0], x_val[:, 1]]

"""Membuat arsitektur model"""

embedding_size = 64

user_input = keras.layers.Input(shape=(1))
user_embedding = keras.layers.Embedding(num_users, embedding_size)(user_input)

movie_input = keras.layers.Input(shape=(1))
movie_embedding = keras.layers.Embedding(num_movies, embedding_size)(movie_input)

mod = keras.layers.Dot(2)([user_embedding, movie_embedding])
mod = keras.layers.Flatten()(mod)
mod = keras.layers.Activation('sigmoid')(mod)

model = Model(inputs=[user_input, movie_input], outputs=mod)

"""Training model"""

model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics=[tf.keras.metrics.RootMeanSquaredError()])

history = model.fit(x=train_inputs, 
                    y=y_train, 
                    validation_data=(val_inputs, y_val),
                    batch_size=1000, 
                    epochs=3, 
                    verbose=1)

"""Dari proses training selama 3 epochs, diperoleh nilai error 0.1557, dan 0.1748 untuk data validasi.

Visualisasi metrik
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""Mendapatkan rekomendasi film"""

user_id = df_fix['user'].sample(1).iloc[0]
movie_watched_by_user = df_fix[df_fix['user'] == user_id]

movie_not_watched = df_fix[~df_fix['movie'].isin(movie_watched_by_user['movie'].values)]['movie'] 
movie_not_watched = list(set(movie_not_watched))
movie_not_watched = [[x] for x in movie_not_watched]

user_movie_array = np.hstack(([[user_id]] * len(movie_not_watched), movie_not_watched))
user_movie_array = [user_movie_array[:, 0], user_movie_array[:, 1]]

"""Film yang pernah ditonton user"""

print('UserId: {}'.format(user_id))
movie_watched_by_user.sort_values(by='rating', ascending=False).head(10)[['title', 'genres', 'rating']]

"""Rekomendasi film untuk user"""

predict = model.predict(user_movie_array).flatten()

top_ratings_indices = predict.argsort()[-20:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watched[x][0]) for x in top_ratings_indices
]

movie_df[movie_df['id'].isin(recommended_movie_ids)][['title', 'genres']]